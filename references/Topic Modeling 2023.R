library(dplyr)
library(stringr)
library(tidyr)
library(tidytext)
library(wordcloud)
library(textstem)
library(igraph)
library(ggraph)
library(gridExtra)
library(stm)
library(tm)

library(Rtsne)#this and the next package were not installed in the first 2 lectures and thus need to be installed here
library(rsvd)

#Importing the r data frame that we created and saved during the last class
#The data should be called "cybersecurity.stemmed.rda"

cybersecurity.stemmed=read.csv("C:/Users/MSI-NB/Documents/R Studio/Raw data/metamorphosis.stemmed.csv")


#So far we have focused our attention on either words or bigrams. But what if
#we would instead like to identify topics? Assume each topic is a distribution over the 
#entire vocabulary representing the corpus. Further, assume the corpus is generated by several
#latent (i.e. hidden) topics. Our job here is to "identify" those latent topics. That is
#the purpose of "topic modeling" in text mining!



#For the interest of time we will only use those words that appear at least 10 times per document
#i.e. we will disregard the relatively "rare" words in a document for this analysis
counts=cybersecurity.stemmed%>%group_by(Paragraph, stemmed)%>%count()

word.counts=counts%>%filter(n>=2)
length(unique(word.counts$stemmed))

#Creating the document-term matrix below, which will be used to estimate the topics
dtm<-word.counts%>%
  cast_dfm(Paragraph, stemmed, n)

#The "stm" function below will estimate the topics. Note that setting "K=0" inside the function tells R that the 
#user does not have a pre-specified number for the topics and wants to get an estimate". 
#It is a preliminary estimate for "T" but will be sufficient for the purposes of our analyses. 
#As is noted by the "stm" creators themselves, "There is not a "right" answer to the
#number of topics that are appropriate for a given corpus" (Roberts, M. E., Stewart, B. M., & Tingley, D. (2014). stm: R package for structural topic models. Journal of Statistical Software, 10(2), 1-40.)

set.seed(123)
topic.model.result<-stm(dtm, 
                K = 0, 
                verbose = FALSE, 
                init.type = "Spectral")


#This will plot top 3 most frequent terms in each topic. The X axis is the expected frequency of each topic. 

plot.STM(topic.model.result, type="summary")

#Words are sized proportional to their use within the plotted topic and oriented along
#the X-axis based on how much they favor one of the two configurations

plot.STM(topic.model.result, type="perspectives", topics=c(3,25))

#Run lines below until "END FUNCTION PLOT.TOP.TERMS" to create a user-defined function to plot the "betas"
#########################################################################################################
#########################################################################################################


topics=tidy(topic.model.result, matrix="beta")
documents=tidy(topic.model.result, matrix="gamma", document_names = rownames(dtm))

top_terms <- topics %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)


plot.top.terms<-function(topicnumber){
  p1=ggplot(top_terms[top_terms$topic==topicnumber,], 
            aes(x=reorder(term, beta), y=beta)) + 
    geom_bar(stat="identity")+xlab("term")+ylab("beta")+
    theme(plot.title = element_text(hjust = 0.5))+
    geom_col(show.legend=FALSE)+
    coord_flip()+ggtitle(paste("Topic Number", topicnumber, sep=" "))
  p1
}
#########################################################################################################
#########################################################################################################

#END FUNCTION PLOT.TOP.TERMS


#Specify which topic to plot below (i.e. the "bettas"), and run. Below we plot the top terms of topic 3
plot.top.terms(20)


#Plotting distribution of topics per document (i.e. thetas)

#Run lines below until "END FUNCTION PLOT.TOP.TOPICS" to create a user-defined function to plot the "thetas"
#########################################################################################################
#########################################################################################################

top_topics <- documents %>%
  group_by(document) %>%
  top_n(10, gamma) %>%
  ungroup() %>%
  arrange(document, -gamma)


plot.top.topics<-function(docnumber){
  numtopics=nrow(as.data.frame(summary(topic.model.result)[1]))
  p1=ggplot(top_topics[top_topics$document==docnumber,], 
            aes(x=topic, y=gamma, fill = as.factor(topic))) + 
    geom_bar(stat="identity")+xlab("topic")+ylab("gamma")+
    theme(plot.title = element_text(hjust = 0.5))+
    geom_col(show.legend=FALSE)+
    ggtitle(paste("Document Number", docnumber, sep=" "))
    
  p1+scale_x_continuous(breaks=sort(top_topics$topic[top_topics==docnumber]))+
    theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank())
}
#########################################################################################################
#########################################################################################################

#"END FUNCTION PLOT.TOP.TOPICS"


#Specify the patent (publication) number to view the
#document-topic distribution below (i.e. the "thetas")
#E.g. view for patent US20160154952A1 (Fitbit)

plot.top.topics(32)


#Now let's plot the top terms in some of the topics identified to be featuring in the Google patent 

plot.top.terms(39)
plot.top.terms(9)




#Acknowledgement: Some code and material in this program was adopted from:

#"Text Mining with R: A Tidy Approach" textbook by Julia Silge and David Robinson, O'Reilly (2017)
#https://www.tidytextmining.com/ 

#https://juliasilge.com/blog/sherlock-holmes-stm/